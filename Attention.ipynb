{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4JcNOtX2j2C+W14gELsXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steenbergen/DeepLearn/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ATTENTION"
      ],
      "metadata": {
        "id": "gUNYyeu1wL29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaELAlzsvzDS",
        "outputId": "bd8810fa-102b-4d39-a0de-5c97e746d1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "826/826 - 9s - loss: 4.8650e-04 - 9s/epoch - 11ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 5s - loss: 3.5576e-04 - 5s/epoch - 6ms/step\n",
            "Epoch 3/30\n",
            "826/826 - 3s - loss: 2.8852e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 4/30\n",
            "826/826 - 3s - loss: 2.0605e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 5/30\n",
            "826/826 - 3s - loss: 1.6594e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/30\n",
            "826/826 - 2s - loss: 1.3600e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 7/30\n",
            "826/826 - 3s - loss: 1.2674e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/30\n",
            "826/826 - 2s - loss: 1.2360e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 9/30\n",
            "826/826 - 3s - loss: 1.0892e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 10/30\n",
            "826/826 - 2s - loss: 1.0670e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 11/30\n",
            "826/826 - 3s - loss: 1.0113e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/30\n",
            "826/826 - 2s - loss: 1.0297e-04 - 2s/epoch - 3ms/step\n",
            "Epoch 13/30\n",
            "826/826 - 3s - loss: 9.4104e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 14/30\n",
            "826/826 - 3s - loss: 9.5664e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 15/30\n",
            "826/826 - 2s - loss: 9.1783e-05 - 2s/epoch - 3ms/step\n",
            "Epoch 16/30\n",
            "826/826 - 3s - loss: 8.1599e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 17/30\n",
            "826/826 - 3s - loss: 9.0756e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 18/30\n",
            "826/826 - 4s - loss: 8.6477e-05 - 4s/epoch - 5ms/step\n",
            "Epoch 19/30\n",
            "826/826 - 3s - loss: 8.6822e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 20/30\n",
            "826/826 - 3s - loss: 8.6570e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 21/30\n",
            "826/826 - 3s - loss: 8.1035e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 22/30\n",
            "826/826 - 4s - loss: 8.4633e-05 - 4s/epoch - 4ms/step\n",
            "Epoch 23/30\n",
            "826/826 - 3s - loss: 8.5779e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 24/30\n",
            "826/826 - 3s - loss: 7.6805e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 25/30\n",
            "826/826 - 3s - loss: 7.9345e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 26/30\n",
            "826/826 - 3s - loss: 7.7856e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 27/30\n",
            "826/826 - 3s - loss: 7.5870e-05 - 3s/epoch - 4ms/step\n",
            "Epoch 28/30\n",
            "826/826 - 3s - loss: 7.5587e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 29/30\n",
            "826/826 - 3s - loss: 7.6492e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 30/30\n",
            "826/826 - 3s - loss: 7.4426e-05 - 3s/epoch - 3ms/step\n",
            "26/26 [==============================] - 1s 4ms/step - loss: 5.3275e-05\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4840e-05\n",
            "Train set MSE =  5.3274761739885435e-05\n",
            "Test set MSE =  2.4840499463607557e-05\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
            "                                                                 \n",
            " attention (attention)       (None, 2)                 22        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "826/826 - 4s - loss: 0.0011 - 4s/epoch - 5ms/step\n",
            "Epoch 2/30\n",
            "826/826 - 3s - loss: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 3/30\n",
            "826/826 - 3s - loss: 0.0011 - 3s/epoch - 3ms/step\n",
            "Epoch 4/30\n",
            "826/826 - 4s - loss: 0.0010 - 4s/epoch - 5ms/step\n",
            "Epoch 5/30\n",
            "826/826 - 3s - loss: 9.5257e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 6/30\n",
            "826/826 - 3s - loss: 9.0462e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 7/30\n",
            "826/826 - 3s - loss: 8.3659e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 8/30\n",
            "826/826 - 3s - loss: 7.5997e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 9/30\n",
            "826/826 - 3s - loss: 6.9551e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 10/30\n",
            "826/826 - 3s - loss: 6.0881e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 11/30\n",
            "826/826 - 3s - loss: 5.3788e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 12/30\n",
            "826/826 - 3s - loss: 4.7280e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 13/30\n",
            "826/826 - 4s - loss: 4.0500e-04 - 4s/epoch - 5ms/step\n",
            "Epoch 14/30\n",
            "826/826 - 3s - loss: 3.5169e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 15/30\n",
            "826/826 - 3s - loss: 2.9105e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 16/30\n",
            "826/826 - 3s - loss: 2.3824e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 17/30\n",
            "826/826 - 4s - loss: 2.0193e-04 - 4s/epoch - 4ms/step\n",
            "Epoch 18/30\n",
            "826/826 - 3s - loss: 1.6957e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 19/30\n",
            "826/826 - 3s - loss: 1.4674e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 20/30\n",
            "826/826 - 3s - loss: 1.3661e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 21/30\n",
            "826/826 - 4s - loss: 1.2380e-04 - 4s/epoch - 5ms/step\n",
            "Epoch 22/30\n",
            "826/826 - 3s - loss: 1.1801e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 23/30\n",
            "826/826 - 3s - loss: 1.1021e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 24/30\n",
            "826/826 - 3s - loss: 1.0524e-04 - 3s/epoch - 3ms/step\n",
            "Epoch 25/30\n",
            "826/826 - 3s - loss: 1.0379e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 26/30\n",
            "826/826 - 3s - loss: 1.0206e-04 - 3s/epoch - 4ms/step\n",
            "Epoch 27/30\n",
            "826/826 - 3s - loss: 9.7122e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 28/30\n",
            "826/826 - 3s - loss: 9.3232e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 29/30\n",
            "826/826 - 3s - loss: 9.2977e-05 - 3s/epoch - 3ms/step\n",
            "Epoch 30/30\n",
            "826/826 - 4s - loss: 8.6951e-05 - 4s/epoch - 5ms/step\n",
            "26/26 [==============================] - 0s 3ms/step - loss: 6.6697e-05\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 1.4670e-05\n",
            "Train set MSE with attention =  6.669749564025551e-05\n",
            "Test set MSE with attention =  1.4670028576801997e-05\n"
          ]
        }
      ],
      "source": [
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense, SimpleRNN\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Prepare data\n",
        "def get_fib_seq(n, scale_data=True):\n",
        "    # Get the Fibonacci sequence\n",
        "    seq = np.zeros(n)\n",
        "    fib_n1 = 0.0\n",
        "    fib_n = 1.0\n",
        "    for i in range(n):\n",
        "            seq[i] = fib_n1 + fib_n\n",
        "            fib_n1 = fib_n\n",
        "            fib_n = seq[i]\n",
        "    scaler = []\n",
        "    if scale_data:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        seq = np.reshape(seq, (n, 1))\n",
        "        seq = scaler.fit_transform(seq).flatten()\n",
        "    return seq, scaler\n",
        "\n",
        "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
        "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)\n",
        "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
        "    Y = dat[Y_ind]\n",
        "    rows_x = len(Y)\n",
        "    X = dat[0:rows_x]\n",
        "    for i in range(time_steps-1):\n",
        "        temp = dat[i+1:rows_x+i+1]\n",
        "        X = np.column_stack((X, temp))\n",
        "    # random permutation with fixed seed\n",
        "    rand = np.random.RandomState(seed=13)\n",
        "    idx = rand.permutation(rows_x)\n",
        "    split = int(train_percent*rows_x)\n",
        "    train_ind = idx[0:split]\n",
        "    test_ind = idx[split:]\n",
        "    trainX = X[train_ind]\n",
        "    trainY = Y[train_ind]\n",
        "    testX = X[test_ind]\n",
        "    testY = Y[test_ind]\n",
        "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))\n",
        "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
        "    return trainX, trainY, testX, testY, scaler\n",
        "\n",
        "# Set up parameters\n",
        "time_steps = 20\n",
        "hidden_units = 2\n",
        "epochs = 30\n",
        "\n",
        "# Create a traditional RNN network\n",
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
        "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1,\n",
        "                       input_shape=(time_steps,1), activation=['tanh', 'tanh'])\n",
        "\n",
        "# Generate the dataset for the network\n",
        "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n",
        "# Train the network\n",
        "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "\n",
        "# Evalute model\n",
        "train_mse = model_RNN.evaluate(trainX, trainY)\n",
        "test_mse = model_RNN.evaluate(testX, testY)\n",
        "\n",
        "# Print error\n",
        "print(\"Train set MSE = \", train_mse)\n",
        "print(\"Test set MSE = \", test_mse)\n",
        "\n",
        "# Add attention layer to the deep learning network\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
        "                               initializer='random_normal', trainable=True)\n",
        "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
        "                               initializer='zeros', trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        # Alignment scores. Pass them through tanh function\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = K.squeeze(e, axis=-1)\n",
        "        # Compute the weights\n",
        "        alpha = K.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = K.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = K.sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
        "    x = Input(shape=input_shape)\n",
        "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
        "    attention_layer = attention()(RNN_layer)\n",
        "    outputs = Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
        "    model = Model(x,outputs)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Create the model with attention, train and evaluate\n",
        "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1,\n",
        "                                  input_shape=(time_steps,1), activation='tanh')\n",
        "model_attention.summary()\n",
        "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "\n",
        "# Evalute model\n",
        "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
        "test_mse_attn = model_attention.evaluate(testX, testY)\n",
        "\n",
        "# Print error\n",
        "print(\"Train set MSE with attention = \", train_mse_attn)\n",
        "print(\"Test set MSE with attention = \", test_mse_attn)\n"
      ]
    }
  ]
}